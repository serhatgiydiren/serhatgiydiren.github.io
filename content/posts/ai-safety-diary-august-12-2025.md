---
id: 931
title: 'AI Safety Diary: August 12, 2025'
date: '2025-08-12T12:24:42+00:00'
author: 'Serhat Giydiren'
layout: post
guid: 'https://serhatgiydiren.com/?p=931'
permalink: /ai-safety-diary-august-12-2025/
categories:
    - 'AI Governance'
    - 'AI Safety'
---

Today, I completed Unit 1: How AI Systems Work of the [BlueDot AI Governance course](https://bluedot.org/courses/governance/1). Below is a summary of each resource I explored.

## Resource: How Does AI Learn? A Beginner’s Guide with Examples

- **Source**: [How Does AI Learn? A Beginner’s Guide with Examples](https://aisafetyfundamentals.com/blog/how-does-ai-learn), AI Safety Fundamentals.
- **Summary**: This guide provides an accessible introduction to how AI systems learn, focusing on machine learning. It explains key concepts like supervised learning (where models learn from labeled data, e.g., identifying spam emails), unsupervised learning (finding patterns in unlabeled data, e.g., clustering customer preferences), and reinforcement learning (learning through rewards, e.g., training a game-playing AI). The article uses simple examples to illustrate how models are trained and highlights challenges like overfitting and the need for diverse datasets to avoid bias.

## Resource: Large Language Models Explained Briefly

- **Source**: [Large Language Models Explained Briefly](https://youtu.be/LPZh9BOjkQs), YouTube video.
- **Summary**: This short video offers a concise overview of large language models (LLMs). It explains that LLMs, like GPT-3, are trained on vast text datasets to predict the next word in a sequence, enabling them to generate coherent text. The video covers their architecture, primarily transformers, and their applications, such as chatbots and text generation. It also touches on limitations, including high computational costs and potential biases in training data.

## Resource: Intro to Large Language Models

- **Source**: [Intro to Large Language Models](https://youtu.be/zjkBMFhNj_g), YouTube video by Andrej Karpathy.
- **Summary**: This one-hour talk provides a detailed introduction to large language models (LLMs). It explains how LLMs are built on transformer architectures and trained on massive text corpora to perform tasks like text generation, translation, and question-answering. The video discusses the importance of scaling compute and data for performance improvements, the emergence of capabilities like reasoning, and challenges such as alignment with human values and mitigating harmful outputs.

## Resource: Visualizing the Deep Learning Revolution

- **Source**: [Visualizing the Deep Learning Revolution](https://medium.com/@richardcngo/visualizing-the-deep-learning-revolution-722098eb9c5) by Richard Ngo, Medium.
- **Summary**: This article illustrates the rapid progress in AI over the past decade, driven by deep learning. It covers advancements in four domains: vision (e.g., image and video generation with GANs, transformers, and diffusion models), games (e.g., AlphaGo, AlphaStar, and AI in open-ended environments like Minecraft), language-based tasks (e.g., GPT-2, GPT-3, and ChatGPT’s text generation and reasoning capabilities), and science (e.g., AlphaFold 2 solving protein folding and AI generating chemical compounds). The article emphasizes that scaling compute and data, rather than new algorithms, has been the primary driver of progress. It also notes that AI’s rapid advancement has surprised experts and raises concerns about existential risks from unaligned AGI.