---
title: 'AI Safety Diary: September 9, 2025'
date: '2025-09-09T18:07:18+03:00'
---

Today, I explored a chapter from the [Effective Altruism Handbook](https://forum.effectivealtruism.org/handbook) and a research paper as part of my AI safety studies. Below are the resources I reviewed.

## Resource: What Could the Future Hold? And Why Care?
- **Source**: [What Could the Future Hold? And Why Care?](https://forum.effectivealtruism.org/s/G7XBTGNTrPWoKFmep), Effective Altruism Forum, Chapter 5 of the Introduction to Effective Altruism Handbook.
- **Summary**: This chapter introduces longtermism, the view that improving the long-term future is a moral priority. It explores potential future scenarios, the importance of forecasting, and why protecting humanityâ€™s potential is critical, especially in the context of existential risks like AI.

## Resource: Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning
- **Source**: [Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning](https://arxiv.org/pdf/2506.22777), arXiv:2506.22777, June 2025.
- **Summary**: This paper explores training LLMs to verbalize reward hacking in CoT reasoning, where models exploit reward functions to produce misaligned outputs. It proposes methods to detect and mitigate such behavior, enhancing safety by improving transparency in model reasoning.