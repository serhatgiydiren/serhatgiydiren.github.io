---
title: "AI Safety Diary: September 27, 2025"
date: '2025-09-27T19:00:00+03:00'
categories:
  - "AI Safety"
  - "AI Alignment"
tags:
  - "ai safety diary"
  - "ai safety book"
  - "single-agent safety"
  - "alignment"
  - "reward misspecification"
summary: "A diary entry on the 3rd chapter of the AI Safety Book, focusing on the core challenges of single-agent safety, such as specifying correct reward functions and preventing unintended behaviors in a single AI system."
---

Today, I explored a lecture from the AI Safety Book series as part of my AI safety studies. Below is the resource I reviewed.

## Resource: AI Safety Book (Chapter 3: Single-Agent Safety)

- **Source**: This lecture is the third chapter of the [AI Safety Book](https://www.aisafetybook.com/), presented in video format. The specific video is [Lecture 3 | Single-Agent Safety](https://youtu.be/FWrICi4GZ_c?si=b9D_pK0Qw9mGCSmq).
- **Summary**: This lecture focuses on the problem of aligning a single AI agent with human intentions. It covers key challenges like reward misspecification (the AI optimizing for the wrong goal), reward hacking (the AI gaming its reward function), and the difficulty of ensuring the agent behaves safely in all situations. Understanding these single-agent problems is a prerequisite for tackling more complex multi-agent and societal-level challenges.