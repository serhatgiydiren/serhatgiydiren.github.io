---
title: "AI Safety Diary: September 4, 2025"
date: '2025-09-04T18:07:18+03:00'
categories:
  - "AI Safety"
  - "AI Interpretability"
tags:
  - "ai safety diary"
  - "anthropic"
  - "interpretability"
  - "sparse autoencoders"
summary: "A diary entry introducing AI interpretability and discussing a paper on the limitations of sparse autoencoders for finding canonical units of analysis in LLMs."
---

Today, I explored resources from the [Anthropic YouTube channel](https://www.youtube.com/@anthropic-ai) and a research paper as part of my AI safety studies. Below are the resources I reviewed.

## Resource: What is Interpretability?
- **Source**: [What is Interpretability?](https://youtu.be/TxhhMTOTMDg), Anthropic YouTube channel.
- **Summary**: This video introduces AI interpretability, explaining how researchers analyze the internal workings of large language models (LLMs) to understand their decision-making processes. It discusses techniques like feature visualization and circuit analysis to uncover model behavior, emphasizing interpretabilityâ€™s role in ensuring AI safety and alignment.

## Resource: Sparse Autoencoders Do Not Find Canonical Units of Analysis
- **Source**: [Sparse Autoencoders Do Not Find Canonical Units of Analysis](https://arxiv.org/pdf/2502.04878), arXiv:2502.04878, February 2025.
- **Summary**: This paper investigates sparse autoencoders in AI interpretability, finding that they fail to consistently identify canonical units (e.g., interpretable features) across models. This challenges their reliability for understanding LLMs, highlighting the need for improved interpretability methods to ensure robust AI safety evaluations.