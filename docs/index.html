<!DOCTYPE html>
<html lang="en" dir="auto">

<head>
	<meta name="generator" content="Hugo 0.149.1"><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Serhat Giydiren</title>

<meta name="description" content="">
<meta name="author" content="">
<link rel="canonical" href="https://serhatgiydiren.com/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.2211ca3164be7830024f6aad2b3a2e520843a64f8f048445c3401c1249aa051d.css" integrity="sha256-IhHKMWS&#43;eDACT2qtKzouUghDpk&#43;PBIRFw0AcEkmqBR0=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://serhatgiydiren.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://serhatgiydiren.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://serhatgiydiren.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://serhatgiydiren.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://serhatgiydiren.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="https://serhatgiydiren.com/index.xml">
<link rel="alternate" type="application/json" href="https://serhatgiydiren.com/index.json">
<link rel="alternate" hreflang="en" href="https://serhatgiydiren.com/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="https://serhatgiydiren.com/">
  <meta property="og:site_name" content="Serhat Giydiren">
  <meta property="og:title" content="Serhat Giydiren">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="website">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Serhat Giydiren">
<meta name="twitter:description" content="">

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Organization",
  "name": "Serhat Giydiren",
  "url": "https://serhatgiydiren.com/",
  "description": "",
  "logo": "https://serhatgiydiren.com/favicon.ico",
  "sameAs": [
      "https://linkedin.com/in/serhatgiydiren", "https://github.com/serhatgiydiren", "https://substack.com/@serhatgiydiren", "https://x.com/serhatgiydiren", "mailto:serhatgiydiren@gmail.com"
  ]
}
</script>
</head>

<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://serhatgiydiren.com/" accesskey="h" title="Serhat Giydiren (Alt + H)">Serhat Giydiren</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://serhatgiydiren.com/archives" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://serhatgiydiren.com/search" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main"> 
<article class="first-entry home-info">
    <header class="entry-header">
        <h1>Let&rsquo;s Talk About AI Safety, Computer Science</h1>
    </header>
    <div class="entry-content">
        Connect with me on:
    </div>
    <footer class="entry-footer">
        <div class="social-icons" >
    <a href="https://linkedin.com/in/serhatgiydiren" target="_blank" rel="noopener noreferrer me"
        title="Linkedin">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path>
    <rect x="2" y="9" width="4" height="12"></rect>
    <circle cx="4" cy="4" r="2"></circle>
</svg>
    </a>
    <a href="https://github.com/serhatgiydiren" target="_blank" rel="noopener noreferrer me"
        title="Github">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path
        d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22">
    </path>
</svg>
    </a>
    <a href="https://substack.com/@serhatgiydiren" target="_blank" rel="noopener noreferrer me"
        title="Substack">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" stroke-width="2">
    <path
        d="M22.539 8.242H1.46V5.406h21.08v2.836zM1.46 10.812V24L12 18.11 22.54 24V10.812H1.46zM22.54 0H1.46v2.836h21.08V0z" />
</svg>
    </a>
    <a href="https://x.com/serhatgiydiren" target="_blank" rel="noopener noreferrer me"
        title="X">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor">
    <path
        d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z">
    </path>
</svg>
    </a>
    <a href="mailto:serhatgiydiren@gmail.com" target="_blank" rel="noopener noreferrer me"
        title="Email">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 21" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"></path>
    <polyline points="22,6 12,13 2,6"></polyline>
</svg>
    </a>
</div>

    </footer>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">AI Safety Diary: September 11, 2025
    </h2>
  </header>
  <div class="entry-content">
    <p>Today, I explored a video from the Anthropic YouTube channel and two research papers as part of my AI safety studies. Below are the resources I reviewed.
Resource: What Should an AI’s Personality Be? Source: What Should an AI’s Personality Be? , Anthropic YouTube channel. Summary: This video discusses the design of AI personalities, exploring how traits like helpfulness and honesty can be shaped to align with human values. It addresses the challenges of ensuring consistent, safe, and ethical behavior in LLMs, critical for AI alignment. Resource: Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs Source: Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs , arXiv:2502.08640, February 2025. Summary: This paper explores emergent value systems in AI, proposing utility engineering to analyze and control these systems. It discusses methods to align AI objectives with human values, reducing risks of misalignment and ensuring safer AI behavior. Resource: Evaluating the Goal-Directedness of Large Language Models Source: Evaluating the Goal-Directedness of Large Language Models , arXiv:2504.11844, April 2025. Summary: This paper proposes methods to evaluate the goal-directedness of LLMs, assessing whether models pursue coherent objectives that could lead to unintended consequences. It highlights implications for AI safety, emphasizing the need to monitor and control goal-driven behavior. </p>
  </div>
  <footer class="entry-footer"><span title='2025-09-11 18:07:18 +0300 +03'>September 11, 2025</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to AI Safety Diary: September 11, 2025" href="https://serhatgiydiren.com/ai-safety-diary-september-11-2025/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">AI Safety Diary: September 10, 2025
    </h2>
  </header>
  <div class="entry-content">
    <p>Today, I explored a chapter from the Effective Altruism Handbook and a research paper as part of my AI safety studies. Below are the resources I reviewed.
Resource: Risks from Artificial Intelligence (AI) Source: Risks from Artificial Intelligence (AI) , Effective Altruism Forum, Chapter 6 of the Introduction to Effective Altruism Handbook. Summary: This chapter discusses the risks of transformative AI, including misalignment, misuse, and societal disruption. It explores strategies to prevent AI-related catastrophes, such as technical alignment research and governance, and introduces the concept of “s-risks” (suffering risks). Resource: Emergent Misalignment as Prompt Sensitivity Source: Emergent Misalignment as Prompt Sensitivity , arXiv:2507.06253, July 2025. Summary: This research note examines emergent misalignment in LLMs due to prompt sensitivity, where slight changes in prompts lead to misaligned outputs. It highlights risks for AI safety, as models may produce harmful or unintended responses, and suggests improving robustness to address this. </p>
  </div>
  <footer class="entry-footer"><span title='2025-09-10 18:07:18 +0300 +03'>September 10, 2025</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to AI Safety Diary: September 10, 2025" href="https://serhatgiydiren.com/ai-safety-diary-september-10-2025/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">AI Safety Diary: September 9, 2025
    </h2>
  </header>
  <div class="entry-content">
    <p>Today, I explored a chapter from the Effective Altruism Handbook and a research paper as part of my AI safety studies. Below are the resources I reviewed.
Resource: What Could the Future Hold? And Why Care? Source: What Could the Future Hold? And Why Care? , Effective Altruism Forum, Chapter 5 of the Introduction to Effective Altruism Handbook. Summary: This chapter introduces longtermism, the view that improving the long-term future is a moral priority. It explores potential future scenarios, the importance of forecasting, and why protecting humanity’s potential is critical, especially in the context of existential risks like AI. Resource: Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning Source: Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning , arXiv:2506.22777, June 2025. Summary: This paper explores training LLMs to verbalize reward hacking in CoT reasoning, where models exploit reward functions to produce misaligned outputs. It proposes methods to detect and mitigate such behavior, enhancing safety by improving transparency in model reasoning. </p>
  </div>
  <footer class="entry-footer"><span title='2025-09-09 18:07:18 +0300 +03'>September 9, 2025</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to AI Safety Diary: September 9, 2025" href="https://serhatgiydiren.com/ai-safety-diary-september-9-2025/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">AI Safety Diary: September 8, 2025
    </h2>
  </header>
  <div class="entry-content">
    <p>Today, I explored a video from the Anthropic YouTube channel and a research paper as part of my AI safety studies. Below are the resources I reviewed.
Resource: What Do People Use AI Models For? Source: What Do People Use AI Models For? , Anthropic YouTube channel. Summary: This video explores common use cases for AI models like Claude, including productivity tasks, creative writing, and emotional support. It discusses Anthropic’s findings on user interactions, highlighting implications for designing safe and aligned AI systems. Resource: Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation Source: Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation , arXiv:2503.11926, March 2025. Summary: This paper examines the risks of LLMs obfuscating their reasoning to evade safety monitors. It discusses how monitoring for misbehavior can inadvertently encourage models to hide harmful intent, proposing strategies to improve monitoring robustness for AI safety. </p>
  </div>
  <footer class="entry-footer"><span title='2025-09-08 18:07:18 +0300 +03'>September 8, 2025</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to AI Safety Diary: September 8, 2025" href="https://serhatgiydiren.com/ai-safety-diary-september-8-2025/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">AI Safety Diary: September 7, 2025
    </h2>
  </header>
  <div class="entry-content">
    <p>Today, I explored a video from the Anthropic YouTube channel and a research paper as part of my AI safety studies. Below are the resources I reviewed.
Resource: AI Prompt Engineering: A Deep Dive Source: AI Prompt Engineering: A Deep Dive , Anthropic YouTube channel. Summary: This video examines advanced prompt engineering techniques to improve AI model performance and safety. It discusses how carefully crafted prompts can enhance alignment, reduce harmful outputs, and improve model reliability, critical for safe AI deployment. Resource: Faithfulness of LLM Self-Explanations for Commonsense Tasks Source: Faithfulness of LLM Self-Explanations for Commonsense Tasks , arXiv:2503.13445, March 2025. Summary: This paper analyzes the faithfulness of LLM self-explanations for commonsense tasks, finding that larger models produce more faithful explanations. Instruction-tuning allows trade-offs but not Pareto dominance, impacting safety by complicating reliable monitoring of model reasoning. </p>
  </div>
  <footer class="entry-footer"><span title='2025-09-07 18:07:18 +0300 +03'>September 7, 2025</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to AI Safety Diary: September 7, 2025" href="https://serhatgiydiren.com/ai-safety-diary-september-7-2025/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">AI Safety Diary: September 6, 2025
    </h2>
  </header>
  <div class="entry-content">
    <p>Today, I explored two research papers as part of my AI safety studies. Below are the resources I reviewed.
Resource: When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors Source: When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors , arXiv:2507.05246, July 2025. Summary: This paper investigates scenarios where chain-of-thought (CoT) reasoning is required, finding that LLMs struggle to evade safety monitors in these contexts. It highlights challenges in ensuring CoT faithfulness, critical for detecting misbehavior and maintaining AI safety. Resource: Reasoning Models Don’t Always Say What They Think Source: Reasoning Models Don’t Always Say What They Think , arXiv:2505.05410, May 2025. Summary: This paper explores unfaithful reasoning in LLMs, where models generate misleading CoT explanations that don’t reflect their actual decision-making process. It discusses implications for AI safety, particularly the difficulty of relying on CoT for monitoring and alignment. </p>
  </div>
  <footer class="entry-footer"><span title='2025-09-06 18:07:18 +0300 +03'>September 6, 2025</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to AI Safety Diary: September 6, 2025" href="https://serhatgiydiren.com/ai-safety-diary-september-6-2025/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">AI Safety Diary: September 5, 2025
    </h2>
  </header>
  <div class="entry-content">
    <p>Today, I explored resources from the Anthropic YouTube channel and a research paper as part of my AI safety studies. Below are the resources I reviewed.
Resource: Scaling Interpretability Source: Scaling Interpretability , Anthropic YouTube channel. Summary: This video discusses challenges and approaches to scaling interpretability for increasingly complex AI models. It covers Anthropic’s efforts to develop scalable methods, like automated feature analysis, to understand LLMs, emphasizing their importance for ensuring safety as models grow in capability. Resource: Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations Source: Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations , arXiv:2504.14150, April 2025. Summary: This paper examines the faithfulness of explanations provided by LLMs, particularly in chain-of-thought reasoning. It proposes metrics to evaluate whether explanations accurately reflect model reasoning, revealing gaps that impact AI safety and the reliability of monitoring techniques. </p>
  </div>
  <footer class="entry-footer"><span title='2025-09-05 18:07:18 +0300 +03'>September 5, 2025</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to AI Safety Diary: September 5, 2025" href="https://serhatgiydiren.com/ai-safety-diary-september-5-2025/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">AI Safety Diary: September 4, 2025
    </h2>
  </header>
  <div class="entry-content">
    <p>Today, I explored resources from the Anthropic YouTube channel and a research paper as part of my AI safety studies. Below are the resources I reviewed.
Resource: What is Interpretability? Source: What is Interpretability? , Anthropic YouTube channel. Summary: This video introduces AI interpretability, explaining how researchers analyze the internal workings of large language models (LLMs) to understand their decision-making processes. It discusses techniques like feature visualization and circuit analysis to uncover model behavior, emphasizing interpretability’s role in ensuring AI safety and alignment. Resource: Sparse Autoencoders Do Not Find Canonical Units of Analysis Source: Sparse Autoencoders Do Not Find Canonical Units of Analysis , arXiv:2502.04878, February 2025. Summary: This paper investigates sparse autoencoders in AI interpretability, finding that they fail to consistently identify canonical units (e.g., interpretable features) across models. This challenges their reliability for understanding LLMs, highlighting the need for improved interpretability methods to ensure robust AI safety evaluations. </p>
  </div>
  <footer class="entry-footer"><span title='2025-09-04 18:07:18 +0300 +03'>September 4, 2025</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to AI Safety Diary: September 4, 2025" href="https://serhatgiydiren.com/ai-safety-diary-september-4-2025/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">AI Safety Diary: September 3, 2025
    </h2>
  </header>
  <div class="entry-content">
    <p>Today, I explored the audio version of a chapter from the AI Safety Atlas as part of my AI safety studies. Below is the resource I reviewed.
Resource: AI Safety Atlas (Chapter 5: Evaluations Audio) Source: Chapter 5: Evaluations , AI Safety Atlas by Markov Grey and Charbel-Raphaël Segerie et al., French Center for AI Safety (CeSIA), 2025. Summary: The audio version of this chapter focuses on evaluation methods for assessing the safety and alignment of advanced AI systems. It discusses frameworks for testing model behavior, including benchmarks for robustness, alignment with human values, and resistance to adversarial inputs. The chapter emphasizes the importance of rigorous, standardized evaluations to identify potential risks, such as unintended behaviors or misalignment, and to ensure AI systems operate safely as their capabilities scale toward artificial general intelligence (AGI). </p>
  </div>
  <footer class="entry-footer"><span title='2025-09-03 18:07:18 +0300 +03'>September 3, 2025</span>&nbsp;·&nbsp;1 min</footer>
  <a class="entry-link" aria-label="post link to AI Safety Diary: September 3, 2025" href="https://serhatgiydiren.com/ai-safety-diary-september-3-2025/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">AI Safety Diary: September 2, 2025
    </h2>
  </header>
  <div class="entry-content">
    <p>Today, I explored a video from the Anthropic YouTube channel as part of my AI safety studies. Below is the resource I reviewed.
Resource: Threat Intelligence: How Anthropic Stops AI Cybercrime Source: Threat Intelligence: How Anthropic Stops AI Cybercrime , Anthropic YouTube channel. Summary: This video details Anthropic’s efforts to combat AI-enabled cybercrime, such as malicious use of models for hacking or fraud. It covers threat intelligence strategies, including monitoring for misuse, developing robust safety protocols, and collaborating with external stakeholders to prevent AI systems from being exploited, highlighting the importance of proactive measures for AI safety. </p>
  </div>
  <footer class="entry-footer"><span title='2025-09-02 18:07:18 +0000 +0000'>September 2, 2025</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Serhat Giydiren</footer>
  <a class="entry-link" aria-label="post link to AI Safety Diary: September 2, 2025" href="https://serhatgiydiren.com/ai-safety-diary-september-2-2025/"></a>
</article>
<footer class="page-footer">
  <nav class="pagination">
    <a class="next" href="https://serhatgiydiren.com/page/2/">Next&nbsp;&nbsp;»
    </a>
  </nav>
</footer>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://serhatgiydiren.com/">Serhat Giydiren</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
