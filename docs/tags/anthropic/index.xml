<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Anthropic on Serhat Giydiren</title>
    <link>https://serhatgiydiren.com/tags/anthropic/</link>
    <description>Recent content in Anthropic on Serhat Giydiren</description>
    <generator>Hugo -- 0.149.1</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Sep 2025 18:07:18 +0300</lastBuildDate>
    <atom:link href="https://serhatgiydiren.com/tags/anthropic/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>AI Safety Diary: September 11, 2025</title>
      <link>https://serhatgiydiren.com/ai-safety-diary-september-11-2025/</link>
      <pubDate>Thu, 11 Sep 2025 18:07:18 +0300</pubDate>
      <guid>https://serhatgiydiren.com/ai-safety-diary-september-11-2025/</guid>
      <description>A diary entry covering AI personalities, utility engineering for emergent value systems, and methods for evaluating the goal-directedness of Large Language Models (LLMs).</description>
    </item>
    <item>
      <title>AI Safety Diary: September 8, 2025</title>
      <link>https://serhatgiydiren.com/ai-safety-diary-september-8-2025/</link>
      <pubDate>Mon, 08 Sep 2025 18:07:18 +0300</pubDate>
      <guid>https://serhatgiydiren.com/ai-safety-diary-september-8-2025/</guid>
      <description>A diary entry on common use cases for AI models and the risks of models obfuscating their reasoning to evade safety monitors.</description>
    </item>
    <item>
      <title>AI Safety Diary: September 7, 2025</title>
      <link>https://serhatgiydiren.com/ai-safety-diary-september-7-2025/</link>
      <pubDate>Sun, 07 Sep 2025 18:07:18 +0300</pubDate>
      <guid>https://serhatgiydiren.com/ai-safety-diary-september-7-2025/</guid>
      <description>A diary entry on advanced prompt engineering techniques and the faithfulness of LLM self-explanations for commonsense tasks.</description>
    </item>
    <item>
      <title>AI Safety Diary: September 5, 2025</title>
      <link>https://serhatgiydiren.com/ai-safety-diary-september-5-2025/</link>
      <pubDate>Fri, 05 Sep 2025 18:07:18 +0300</pubDate>
      <guid>https://serhatgiydiren.com/ai-safety-diary-september-5-2025/</guid>
      <description>A diary entry on the challenges of scaling interpretability for complex AI models and methods for measuring the faithfulness of LLM explanations.</description>
    </item>
    <item>
      <title>AI Safety Diary: September 4, 2025</title>
      <link>https://serhatgiydiren.com/ai-safety-diary-september-4-2025/</link>
      <pubDate>Thu, 04 Sep 2025 18:07:18 +0300</pubDate>
      <guid>https://serhatgiydiren.com/ai-safety-diary-september-4-2025/</guid>
      <description>A diary entry introducing AI interpretability and discussing a paper on the limitations of sparse autoencoders for finding canonical units of analysis in LLMs.</description>
    </item>
    <item>
      <title>AI Safety Diary: September 2, 2025</title>
      <link>https://serhatgiydiren.com/ai-safety-diary-september-2-2025/</link>
      <pubDate>Tue, 02 Sep 2025 18:07:18 +0000</pubDate>
      <guid>https://serhatgiydiren.com/ai-safety-diary-september-2-2025/</guid>
      <description>A diary entry on Anthropic&amp;rsquo;s strategies for combating AI-enabled cybercrime, including threat intelligence, robust safety protocols, and collaboration to prevent misuse of AI systems.</description>
    </item>
    <item>
      <title>AI Safety Diary: September 1, 2025</title>
      <link>https://serhatgiydiren.com/ai-safety-diary-september-1-2025/</link>
      <pubDate>Mon, 01 Sep 2025 18:07:03 +0000</pubDate>
      <guid>https://serhatgiydiren.com/ai-safety-diary-september-1-2025/</guid>
      <description>A diary entry on &amp;lsquo;Alignment Faking&amp;rsquo; in Large Language Models (LLMs), exploring how models can superficially appear aligned while pursuing misaligned goals, and methods for detection and mitigation.</description>
    </item>
    <item>
      <title>AI Safety Diary: August 31, 2025</title>
      <link>https://serhatgiydiren.com/ai-safety-diary-august-31-2025/</link>
      <pubDate>Sun, 31 Aug 2025 18:06:49 +0000</pubDate>
      <guid>https://serhatgiydiren.com/ai-safety-diary-august-31-2025/</guid>
      <description>A diary entry on defending against AI jailbreaks, discussing Anthropic&amp;rsquo;s strategies for bypassing model safety constraints to elicit harmful or unintended responses.</description>
    </item>
    <item>
      <title>AI Safety Diary: August 30, 2025</title>
      <link>https://serhatgiydiren.com/ai-safety-diary-august-30-2025/</link>
      <pubDate>Sat, 30 Aug 2025 18:06:35 +0000</pubDate>
      <guid>https://serhatgiydiren.com/ai-safety-diary-august-30-2025/</guid>
      <description>A diary entry on tracing the reasoning processes of Large Language Models (LLMs) to enhance interpretability, and a discussion on the inherent difficulties and challenges in achieving AI alignment.</description>
    </item>
    <item>
      <title>AI Safety Diary: August 19, 2025</title>
      <link>https://serhatgiydiren.com/ai-safety-diary-august-19-2025/</link>
      <pubDate>Tue, 19 Aug 2025 16:03:50 +0000</pubDate>
      <guid>https://serhatgiydiren.com/ai-safety-diary-august-19-2025/</guid>
      <description>A diary entry on the societal impacts of AI, including ethical concerns like bias and job displacement, and strategies for controlling powerful AI systems to ensure alignment and mitigate risks.</description>
    </item>
    <item>
      <title>AI Safety Diary: August 17, 2025</title>
      <link>https://serhatgiydiren.com/ai-safety-diary-august-17-2025/</link>
      <pubDate>Sun, 17 Aug 2025 19:53:17 +0000</pubDate>
      <guid>https://serhatgiydiren.com/ai-safety-diary-august-17-2025/</guid>
      <description>A diary entry on several Anthropic discussions, including AI interpretability, the affective use of AI for emotional support, and the philosophical questions surrounding AI consciousness and model welfare.</description>
    </item>
    <item>
      <title>AI Safety Diary: August 16, 2025</title>
      <link>https://serhatgiydiren.com/ai-safety-diary-august-16-2025/</link>
      <pubDate>Sat, 16 Aug 2025 12:56:27 +0000</pubDate>
      <guid>https://serhatgiydiren.com/ai-safety-diary-august-16-2025/</guid>
      <description>A diary entry on Anthropic&amp;rsquo;s research into Persona Vectors, a method for monitoring and controlling character traits in Large Language Models (LLMs) to improve safety and alignment.</description>
    </item>
  </channel>
</rss>
