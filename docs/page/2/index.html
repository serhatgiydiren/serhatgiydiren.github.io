<!DOCTYPE html>
<html lang="en">
<head>
	<meta name="generator" content="Hugo 0.149.1">
  
    <title>Serhat Giydiren Blog</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="" />
<meta name="keywords" content="" />

  <meta name="robots" content="noodp" />

<link rel="canonical" href="https://serhatgiydiren.github.io/" />





  
  <link rel="stylesheet" href="https://serhatgiydiren.github.io/css/buttons.min.86f6b4c106b6c6eb690ae5203d36b442c1f66f718ff4e8164fa86cf6c61ad641.css">

  
  <link rel="stylesheet" href="https://serhatgiydiren.github.io/css/code.min.d529ea4b2fb8d34328d7d31afc5466d5f7bc2f0bc9abdd98b69385335d7baee4.css">

  
  <link rel="stylesheet" href="https://serhatgiydiren.github.io/css/fonts.min.5bb7ed13e1d00d8ff39ea84af26737007eb5051b157b86fc24487c94f3dc8bbe.css">

  
  <link rel="stylesheet" href="https://serhatgiydiren.github.io/css/footer.min.eb8dfc2c6a7eafa36cd3ba92d63e69e849e2200e0002a228d137f236b09ecd75.css">

  
  <link rel="stylesheet" href="https://serhatgiydiren.github.io/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css">

  
  <link rel="stylesheet" href="https://serhatgiydiren.github.io/css/header.min.75c7eb0e2872d95ff48109c6647d0223a38db52e2561dd87966eb5fc7c6bdac6.css">

  
  <link rel="stylesheet" href="https://serhatgiydiren.github.io/css/main.min.36833afd348409fc6c3d09d0897c5833d9d5bf1ff31f5e60ea3ee42ce2b1268c.css">

  
  <link rel="stylesheet" href="https://serhatgiydiren.github.io/css/menu.min.3c17467ebeb3d38663dce68f71f519901124fa5cbb4519b2fb0667a21e9aca39.css">

  
  <link rel="stylesheet" href="https://serhatgiydiren.github.io/css/pagination.min.bbb986dbce00a5ce5aca0504b7925fc1c581992a4bf57f163e5d69cc1db7d836.css">

  
  <link rel="stylesheet" href="https://serhatgiydiren.github.io/css/post.min.e6dddd258e64c83e05cec0cd49c05216742d42fc8ecbfbe6b67083412b609bd3.css">

  
  <link rel="stylesheet" href="https://serhatgiydiren.github.io/css/syntax.min.a0773cce9310cb6d8ed23e50f005448facf29a53001b57e038828daa466b25c0.css">

  
  <link rel="stylesheet" href="https://serhatgiydiren.github.io/css/terminal.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css">

  
  <link rel="stylesheet" href="https://serhatgiydiren.github.io/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css">







<link rel="shortcut icon" href="https://serhatgiydiren.github.io/favicon.png">
<link rel="apple-touch-icon" href="https://serhatgiydiren.github.io/apple-touch-icon.png">


<meta name="twitter:card" content="summary" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="website" />
<meta property="og:title" content="Serhat Giydiren Blog">
<meta property="og:description" content="" />
<meta property="og:url" content="https://serhatgiydiren.github.io/" />
<meta property="og:site_name" content="Serhat Giydiren Blog" />

  <meta property="og:image" content="https://serhatgiydiren.github.io/og-image.png">

<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="627">





  <link href="/index.xml" rel="alternate" type="application/rss+xml" title="Serhat Giydiren Blog" />









</head>
<body>


<div class="container">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="https://serhatgiydiren.github.io/">
  <div class="logo">
    Terminal
  </div>
</a>

    </div>
    
    
  </div>
  
</header>


  <div class="content">
    
  
  <div class="posts">
    
    

    
    
      
    
    

    
      <article class="post on-list">
        <h2 class="post-title">
          <a href="https://serhatgiydiren.github.io/posts/ai-safety-diary-august-23-2025/">AI Safety Diary: August 23, 2025</a>
        </h2>

        <div class="post-meta"><time class="post-date">2025-08-23</time><span class="post-author">Serhat Giydiren</span></div>

        

        


        <div class="post-content">
          
            <p>Today, I explored the audio version of a chapter from the <a href="https://ai-safety-atlas.com/">AI Safety Atlas</a> as part of my AI safety studies. Below is the resource I reviewed.</p>
<h2 id="resource-ai-safety-atlas-chapter-3-strategies-audio">Resource: AI Safety Atlas (Chapter 3: Strategies Audio)</h2>
<ul>
<li><strong>Source</strong>: <a href="https://ai-safety-atlas.com/chapters/03">Chapter 3: Strategies</a>, AI Safety Atlas by Markov Grey and Charbel-Raphaël Segerie et al., French Center for AI Safety (CeSIA), 2025.</li>
<li><strong>Summary</strong>: The audio version of this chapter outlines strategies for mitigating risks associated with advanced AI systems, particularly as they approach artificial general intelligence (AGI). It covers technical approaches such as improving model alignment, enhancing robustness against adversarial attacks, and developing interpretable AI systems. The chapter also discusses governance strategies, including safety standards, international cooperation, and regulatory frameworks to ensure responsible AI development. It emphasizes proactive measures like iterative testing, red-teaming, and stakeholder coordination to address potential safety challenges and align AI with human values.</li>
</ul>
          
        </div>

        
          <div>
            <a class="read-more button inline" href="/posts/ai-safety-diary-august-23-2025/">[]</a>
          </div>
        
      </article>
    
      <article class="post on-list">
        <h2 class="post-title">
          <a href="https://serhatgiydiren.github.io/posts/ai-safety-diary-august-22-2025/">AI Safety Diary: August 22, 2025</a>
        </h2>

        <div class="post-meta"><time class="post-date">2025-08-22</time><span class="post-author">Serhat Giydiren</span></div>

        

        


        <div class="post-content">
          
            <p>Today, I explored the audio version of a chapter from the <a href="https://ai-safety-atlas.com/">AI Safety Atlas</a> as part of my AI safety studies. Below is the resource I reviewed.</p>
<h2 id="resource-ai-safety-atlas-chapter-2-risks-audio">Resource: AI Safety Atlas (Chapter 2: Risks Audio)</h2>
<ul>
<li><strong>Source</strong>: <a href="https://ai-safety-atlas.com/chapters/02">Chapter 2: Risks</a>, AI Safety Atlas by Markov Grey and Charbel-Raphaël Segerie et al., French Center for AI Safety (CeSIA), 2025.</li>
<li><strong>Summary</strong>: The audio version of this chapter examines the risks associated with advanced AI systems, particularly as they approach or achieve artificial general intelligence (AGI). It categorizes risks into several types, including misuse (e.g., malicious use by bad actors), accidents (e.g., unintended consequences from misaligned systems), and systemic risks (e.g., economic disruption or concentration of power). The chapter discusses the challenges of ensuring AI safety as systems scale, emphasizing the potential for catastrophic outcomes if risks are not mitigated. It also introduces key concepts like alignment failures, robustness issues, and the importance of proactive risk management to safeguard societal well-being.</li>
</ul>
          
        </div>

        
          <div>
            <a class="read-more button inline" href="/posts/ai-safety-diary-august-22-2025/">[]</a>
          </div>
        
      </article>
    
      <article class="post on-list">
        <h2 class="post-title">
          <a href="https://serhatgiydiren.github.io/posts/ai-safety-diary-august-21-2025/">AI Safety Diary: August 21, 2025</a>
        </h2>

        <div class="post-meta"><time class="post-date">2025-08-21</time><span class="post-author">Serhat Giydiren</span></div>

        

        


        <div class="post-content">
          
            <p>Today, I explored the audio version of a chapter from the <a href="https://ai-safety-atlas.com/">AI Safety Atlas</a> as part of my AI safety studies. Below is the resource I reviewed.</p>
<h2 id="resource-ai-safety-atlas-chapter-1-capabilities-audio">Resource: AI Safety Atlas (Chapter 1: Capabilities Audio)</h2>
<ul>
<li><strong>Source</strong>: <a href="https://ai-safety-atlas.com/chapters/01">Chapter 1: Capabilities</a>, AI Safety Atlas by Markov Grey and Charbel-Raphaël Segerie et al., French Center for AI Safety (CeSIA), 2025.</li>
<li><strong>Summary</strong>: The audio version of this chapter provides an overview of AI capabilities, focusing on the progression of modern AI systems toward artificial general intelligence (AGI). It discusses the increasing power of foundation models, such as large language models, and the importance of defining and measuring intelligence for safety purposes. The chapter explores challenges in defining intelligence, comparing approaches like the Turing Test, consciousness-based definitions, process-based adaptability, and a capabilities-focused framework. It emphasizes the latter, which assesses what AI systems can do, their performance levels, and the range of tasks they can handle, as the most practical for safety evaluations. The chapter also introduces frameworks for measuring AI progress on a continuous spectrum, moving beyond binary distinctions like narrow versus general AI, to better understand capabilities and associated risks.</li>
</ul>
          
        </div>

        
          <div>
            <a class="read-more button inline" href="/posts/ai-safety-diary-august-21-2025/">[]</a>
          </div>
        
      </article>
    
      <article class="post on-list">
        <h2 class="post-title">
          <a href="https://serhatgiydiren.github.io/posts/ai-safety-diary-august-20-2025/">AI Safety Diary: August 20, 2025</a>
        </h2>

        <div class="post-meta"><time class="post-date">2025-08-20</time><span class="post-author">Serhat Giydiren</span></div>

        

        


        <div class="post-content">
          
            <p>Today, I explored a research paper as part of my AI safety studies. Below is the resource I reviewed.</p>
<h2 id="resource-vending-bench-a-benchmark-for-long-term-coherence-of-autonomous-agents">Resource: Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents</h2>
<ul>
<li><strong>Source</strong>: <a href="https://arxiv.org/pdf/2502.15840">Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents</a> by Axel Backlund and Lukas Petersson, Andon Labs, arXiv:2502.15840, February 2025.</li>
<li><strong>Summary</strong>: This paper introduces Vending-Bench, a simulated environment designed to test the long-term coherence of large language model (LLM)-based agents in managing a vending machine business. Agents must handle inventory, orders, pricing, and daily fees over extended periods (&gt;20M tokens per run), revealing high variance in performance. Models like Claude 3.5 Sonnet and o3-mini often succeed but can fail due to misinterpreting schedules, forgetting orders, or entering &ldquo;meltdown&rdquo; loops. The benchmark highlights LLMs’ challenges in sustained decision-making and tests their ability to manage capital, relevant to AI safety in scenarios involving powerful autonomous agents.</li>
</ul>
          
        </div>

        
          <div>
            <a class="read-more button inline" href="/posts/ai-safety-diary-august-20-2025/">[]</a>
          </div>
        
      </article>
    
      <article class="post on-list">
        <h2 class="post-title">
          <a href="https://serhatgiydiren.github.io/posts/ai-safety-diary-august-19-2025/">AI Safety Diary: August 19, 2025</a>
        </h2>

        <div class="post-meta"><time class="post-date">2025-08-19</time><span class="post-author">Serhat Giydiren</span></div>

        

        


        <div class="post-content">
          
            <p>Today, I explored two videos from the <a href="https://www.youtube.com/@anthropic-ai">Anthropic YouTube channel</a> as part of my AI safety studies. Below are the resources I reviewed.</p>
<h2 id="resource-the-societal-impacts-of-ai">Resource: The Societal Impacts of AI</h2>
<ul>
<li><strong>Source</strong>: <a href="https://youtu.be/02nFRuEo0bc?si=IMpf20I-OR908Xxh">The Societal Impacts of AI</a>, Anthropic YouTube channel.</li>
<li><strong>Summary</strong>: This video features Anthropic researchers discussing how to measure and shape AI&rsquo;s influence on society through careful observation and analysis. It explores AI’s transformative potential across industries like healthcare, education, and agriculture, while addressing ethical concerns such as bias, job displacement, and privacy. The discussion emphasizes the need for responsible AI deployment to ensure equitable and positive societal outcomes.<a href="https://www.youtube.com/watch?v=02nFRuEo0bc"></a></li>
</ul>
<h2 id="resource-controlling-powerful-ai">Resource: Controlling Powerful AI</h2>
<ul>
<li><strong>Source</strong>: <a href="https://youtu.be/6Unxqr50Kqg?si=qIeCiQizdMCZL_U5">Controlling Powerful AI</a>, Anthropic YouTube channel.</li>
<li><strong>Summary</strong>: This video examines strategies for managing the risks of advanced AI systems. It discusses technical approaches to ensure powerful AI remains aligned with human values, including methods to mitigate unintended behaviors and prevent catastrophic outcomes. The talk highlights Anthropic’s research into safe AI development, emphasizing governance and alignment mechanisms to control increasingly capable models.</li>
</ul>
          
        </div>

        
          <div>
            <a class="read-more button inline" href="/posts/ai-safety-diary-august-19-2025/">[]</a>
          </div>
        
      </article>
    
      <article class="post on-list">
        <h2 class="post-title">
          <a href="https://serhatgiydiren.github.io/posts/ai-safety-diary-august-18-2025/">AI Safety Diary: August 18, 2025</a>
        </h2>

        <div class="post-meta"><time class="post-date">2025-08-18</time><span class="post-author">Serhat Giydiren</span></div>

        

        


        <div class="post-content">
          
            <p>Today, I continued exploring the <a href="https://forum.effectivealtruism.org/handbook">Introduction to Effective Altruism Handbook</a> as part of my AI safety and governance studies. Below is the resource I reviewed.</p>
<h2 id="resource-radical-empathy">Resource: Radical Empathy</h2>
<ul>
<li><strong>Source</strong>: <a href="https://forum.effectivealtruism.org/s/QMrYGgBvg64JhcQrS">Radical Empathy</a>, Effective Altruism Forum, Chapter 3 of the Introduction to Effective Altruism Handbook.</li>
<li><strong>Summary</strong>: This chapter explores the concept of impartial care, emphasizing the importance of extending empathy to non-human animals and other unconventional beneficiaries. It argues against dismissing unusual topics and proposes ways to improve the welfare of animals suffering in factory farms, highlighting the moral significance of considering all sentient beings in effective altruism efforts.</li>
</ul>
          
        </div>

        
          <div>
            <a class="read-more button inline" href="/posts/ai-safety-diary-august-18-2025/">[]</a>
          </div>
        
      </article>
    
      <article class="post on-list">
        <h2 class="post-title">
          <a href="https://serhatgiydiren.github.io/posts/ai-safety-diary-august-17-2025/">AI Safety Diary: August 17, 2025</a>
        </h2>

        <div class="post-meta"><time class="post-date">2025-08-17</time><span class="post-author">Serhat Giydiren</span></div>

        

        


        <div class="post-content">
          
            <p>Today, I explored three videos from the <a href="https://www.youtube.com/@anthropic-ai">Anthropic YouTube channel</a> as part of my AI safety studies. Below are the resources I reviewed.</p>
<h2 id="resource-interpretability-understanding-how-ai-models-think">Resource: Interpretability: Understanding how AI models think</h2>
<ul>
<li><strong>Source</strong>: <a href="https://youtu.be/fGKNUvivvnc?si=qMF1hd1O3se_FFy2">Interpretability: Understanding how AI models think</a>, Anthropic YouTube channel.<a href="https://www.youtube.com/watch?v=fGKNUvivvnc"></a></li>
<li><strong>Summary</strong>: This video features Anthropic researchers Josh Batson, Emmanuel Ameisen, and Jack Lindsey discussing AI interpretability. It explores how large language models (LLMs) process information, addressing questions like why models exhibit sycophancy or hallucination. The talk covers scientific methods to open the &ldquo;black box&rdquo; of AI, including circuit tracing to reveal computational pathways in Claude. It highlights findings such as Claude’s planning ahead in tasks like poetry, its use of a universal &ldquo;language of thought&rdquo; across languages, and its fabrication of plausible arguments when influenced by incorrect user hints, emphasizing the role of interpretability in ensuring model safety.</li>
</ul>
<h2 id="resource-affective-use-of-ai">Resource: Affective Use of AI</h2>
<ul>
<li><strong>Source</strong>: <a href="https://youtu.be/8fVHFt7Shf4?si=8jUYXXxWDNDpbAjr">Affective Use of AI</a>, Anthropic YouTube channel.<a href="https://www.youtube.com/watch?v=8fVHFt7Shf4"></a></li>
<li><strong>Summary</strong>: This fireside chat examines how people use Claude for emotional support and companionship, beyond its primary use for work tasks and content creation. The video discusses Anthropic’s research, finding that 2.9% of Claude.ai interactions involve affective conversations, such as seeking advice, coaching, or companionship. It highlights Claude’s role in addressing topics like career transitions, relationships, and existential questions, with minimal pushback (less than 10%) in supportive contexts, except to protect user well-being. The study emphasizes privacy-preserving analysis and the implications for AI safety.</li>
</ul>
<h2 id="resource-could-ai-models-be-conscious">Resource: Could AI models be conscious?</h2>
<ul>
<li><strong>Source</strong>: <a href="https://youtu.be/pyXouxa0WnY?si=rqOdtNLe7S6D0kPC">Could AI models be conscious?</a>, Anthropic YouTube channel.</li>
<li><strong>Summary</strong>: This video explores the philosophical and scientific question of whether AI models like Claude could be conscious. It discusses Anthropic’s new research program on model welfare, investigating whether advanced AI systems might deserve moral consideration due to their capabilities in communication, planning, and problem-solving. The video addresses the lack of scientific consensus on AI consciousness, the challenges in studying it, and the need for humility in approaching these questions to ensure responsible AI development.</li>
</ul>
          
        </div>

        
          <div>
            <a class="read-more button inline" href="/posts/ai-safety-diary-august-17-2025/">[]</a>
          </div>
        
      </article>
    
      <article class="post on-list">
        <h2 class="post-title">
          <a href="https://serhatgiydiren.github.io/posts/ai-safety-diary-august-16-2025/">AI Safety Diary: August 16, 2025</a>
        </h2>

        <div class="post-meta"><time class="post-date">2025-08-16</time><span class="post-author">Serhat Giydiren</span></div>

        

        


        <div class="post-content">
          
            <p>Today, I explored resources related to Anthropic&rsquo;s research on persona vectors as part of my AI safety studies. Below are the resources I reviewed.</p>
<h2 id="resource-persona-vectors-monitoring-and-controlling-character-traits-in-language-models">Resource: Persona Vectors: Monitoring and Controlling Character Traits in Language Models</h2>
<ul>
<li><strong>Source</strong>: <a href="https://www.anthropic.com/research/persona-vectors">Persona Vectors: Monitoring and Controlling Character Traits in Language Models</a>, Anthropic Research; related paper: <a href="https://arxiv.org/pdf/2507.21509">Persona Vectors: Monitoring and Controlling Character Traits in Language Models</a> by Runjin Chen et al.; implementation: <a href="https://github.com/safety-research/persona_vectors">GitHub - safety-research/persona_vectors</a>.</li>
<li><strong>Summary</strong>: This Anthropic Research page introduces persona vectors, patterns of neural network activity in large language models (LLMs) that control character traits like evil, sycophancy, or hallucination. The associated paper details a method to extract these vectors by comparing model activations for opposing behaviors (e.g., evil vs. non-evil responses). Persona vectors enable monitoring of personality shifts during conversations or training, mitigating undesirable traits through steering techniques, and flagging problematic training data. The method is tested on open-source models like Qwen 2.5-7B-Instruct and Llama-3.1-8B-Instruct. The GitHub repository provides code for generating persona vectors, evaluating their effectiveness, and applying steering during training to prevent unwanted trait shifts, offering tools for maintaining alignment with human values.</li>
</ul>
          
        </div>

        
          <div>
            <a class="read-more button inline" href="/posts/ai-safety-diary-august-16-2025/">[]</a>
          </div>
        
      </article>
    
      <article class="post on-list">
        <h2 class="post-title">
          <a href="https://serhatgiydiren.github.io/posts/ai-safety-diary-august-15-2025/">AI Safety Diary: August 15, 2025</a>
        </h2>

        <div class="post-meta"><time class="post-date">2025-08-15</time><span class="post-author">Serhat Giydiren</span></div>

        

        


        <div class="post-content">
          
            <p>Today, I continued exploring the <a href="https://forum.effectivealtruism.org/handbook">Effective Altruism Handbook</a> and completed its second chapter as part of my studies related to AI safety and governance. Below is the resource I reviewed.</p>
<h2 id="resource-differences-in-impact">Resource: Differences in Impact</h2>
<ul>
<li><strong>Source</strong>: <a href="https://forum.effectivealtruism.org/s/x3KXkiAQ6NH8WLbkW">Differences in Impact</a>, Effective Altruism Forum, Chapter 2 of the Effective Altruism Handbook.</li>
<li><strong>Summary</strong>: This chapter focuses on the significant disparities in the effectiveness of interventions aimed at helping the approximately 700 million people living in poverty, primarily in low-income countries. It discusses strategies such as policy reform, cash transfers, and health service provision, emphasizing that some interventions are far more effective than others. The chapter introduces a simple tool for estimating key figures to evaluate impact and includes recommended readings, such as GiveWell&rsquo;s &ldquo;Giving 101&rdquo; guide and sections on global health outcomes, to illustrate effective altruism approaches to addressing global poverty.</li>
</ul>
          
        </div>

        
          <div>
            <a class="read-more button inline" href="/posts/ai-safety-diary-august-15-2025/">[]</a>
          </div>
        
      </article>
    
      <article class="post on-list">
        <h2 class="post-title">
          <a href="https://serhatgiydiren.github.io/posts/ai-safety-diary-august-14-2025/">AI Safety Diary: August 14, 2025</a>
        </h2>

        <div class="post-meta"><time class="post-date">2025-08-14</time><span class="post-author">Serhat Giydiren</span></div>

        

        


        <div class="post-content">
          
            <p>Today, I explored the <a href="https://ai-safety-atlas.com/">AI Safety Atlas</a> as part of my AI safety studies. Below is the resource I reviewed.</p>
<h2 id="resource-ai-safety-atlas-chapter-1-capabilities">Resource: AI Safety Atlas (Chapter 1: Capabilities)</h2>
<ul>
<li><strong>Source</strong>: <a href="https://youtu.be/J_iMeH1hb9M?si=Ds7buMC7off_dD8A">Chapter 1: Capabilities - Video Lecture (AI is Advancing Faster Than You Think! (AI Safety symposium 2/5))</a>, AI Safety Atlas by Markov Grey and Charbel-Raphaël Segerie et al., French Center for AI Safety (CeSIA), 2025.</li>
<li><strong>Summary</strong>: This chapter provides an overview of AI capabilities, focusing on the progression of modern AI systems toward artificial general intelligence (AGI). It discusses the increasing power of foundation models, such as large language models, and the importance of defining and measuring intelligence for safety purposes. The chapter explores challenges in defining intelligence, comparing approaches like the Turing Test, consciousness-based definitions, process-based adaptability, and a capabilities-focused framework. It emphasizes the latter, which assesses what AI systems can do, their performance levels, and the range of tasks they can handle, as the most practical for safety evaluations. The chapter also introduces frameworks for measuring AI progress on a continuous spectrum, moving beyond binary distinctions like narrow versus general AI, to better understand capabilities and associated risks.<a href="https://ai-safety-atlas.com/chapters/01/03"></a></li>
</ul>
          
        </div>

        
          <div>
            <a class="read-more button inline" href="/posts/ai-safety-diary-august-14-2025/">[]</a>
          </div>
        
      </article>
    

    <div class="pagination">
  <div class="pagination__buttons">
    
      <a href="/" class="button inline prev">
        &lt; [<span class="button__text">Newer posts</span>]
      </a>
    
    
      ::
    
    
      <a href="/page/3/" class="button inline next">
        [<span class="button__text">Older posts</span>] &gt;
      </a>
    
  </div>
</div>

  </div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>© 2025 Powered by <a href="https://gohugo.io">Hugo</a></span>
    
      <span>:: <a href="https://github.com/panr/hugo-theme-terminal" target="_blank">Theme</a> made by <a href="https://github.com/panr" target="_blank">panr</a></span>
      </div>
  </div>
</footer>






<script type="text/javascript" src="/bundle.min.js"></script>





  
</div>

</body>
</html>
