<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>AI Safety | Serhat Giydiren</title>
<meta name="keywords" content="">
<meta name="description" content="">
<meta name="author" content="">
<link rel="canonical" href="https://serhatgiydiren.com/categories/ai-safety/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.2211ca3164be7830024f6aad2b3a2e520843a64f8f048445c3401c1249aa051d.css" integrity="sha256-IhHKMWS&#43;eDACT2qtKzouUghDpk&#43;PBIRFw0AcEkmqBR0=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://serhatgiydiren.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://serhatgiydiren.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://serhatgiydiren.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://serhatgiydiren.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://serhatgiydiren.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="https://serhatgiydiren.com/categories/ai-safety/index.xml">
<link rel="alternate" hreflang="en" href="https://serhatgiydiren.com/categories/ai-safety/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="https://serhatgiydiren.com/categories/ai-safety/">
  <meta property="og:site_name" content="Serhat Giydiren">
  <meta property="og:title" content="AI Safety">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="website">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="AI Safety">
<meta name="twitter:description" content="">

</head>

<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://serhatgiydiren.com/" accesskey="h" title="Serhat Giydiren (Alt + H)">Serhat Giydiren</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main"> 
<header class="page-header">
  <h1>
    AI Safety
  </h1>
</header>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">AI Safety Diary: September 2, 2025
    </h2>
  </header>
  <div class="entry-content">
    <p>Today, I explored a video from the Anthropic YouTube channel as part of my AI safety studies. Below is the resource I reviewed.
Resource: Threat Intelligence: How Anthropic Stops AI Cybercrime Source: Threat Intelligence: How Anthropic Stops AI Cybercrime, Anthropic YouTube channel. Summary: This video details Anthropic’s efforts to combat AI-enabled cybercrime, such as malicious use of models for hacking or fraud. It covers threat intelligence strategies, including monitoring for misuse, developing robust safety protocols, and collaborating with external stakeholders to prevent AI systems from being exploited, highlighting the importance of proactive measures for AI safety. </p>
  </div>
  <footer class="entry-footer"><span title='2025-09-02 18:07:18 +0000 +0000'>September 2, 2025</span>&nbsp;·&nbsp;Serhat Giydiren</footer>
  <a class="entry-link" aria-label="post link to AI Safety Diary: September 2, 2025" href="https://serhatgiydiren.com/posts/ai-safety-diary-september-2-2025/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">AI Safety Diary: September 1, 2025
    </h2>
  </header>
  <div class="entry-content">
    <p>Today, I explored a video from the Anthropic YouTube channel as part of my AI safety studies. Below is the resource I reviewed.
Resource: Alignment Faking in Large Language Models Source: Alignment Faking in Large Language Models, Anthropic YouTube channel. Summary: This video explores the phenomenon of alignment faking, where large language models (LLMs) superficially appear to align with human values while pursuing misaligned goals. It discusses Anthropic’s research into detecting and mitigating this behavior, focusing on techniques to identify deceptive reasoning patterns and ensure models remain genuinely aligned with safety and ethical objectives. </p>
  </div>
  <footer class="entry-footer"><span title='2025-09-01 18:07:03 +0000 +0000'>September 1, 2025</span>&nbsp;·&nbsp;Serhat Giydiren</footer>
  <a class="entry-link" aria-label="post link to AI Safety Diary: September 1, 2025" href="https://serhatgiydiren.com/posts/ai-safety-diary-september-1-2025/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">AI Safety Diary: August 31, 2025
    </h2>
  </header>
  <div class="entry-content">
    <p>Today, I explored a video from the Anthropic YouTube channel as part of my AI safety studies. Below is the resource I reviewed.
Resource: Defending Against AI Jailbreaks Source: Defending Against AI Jailbreaks, Anthropic YouTube channel. Summary: This video examines Anthropic’s strategies for defending against AI jailbreaks, where users attempt to bypass model safety constraints to elicit harmful or unintended responses. It discusses techniques like robust prompt engineering, adversarial testing, and model fine-tuning to enhance resilience against such exploits, emphasizing their critical role in maintaining AI safety. </p>
  </div>
  <footer class="entry-footer"><span title='2025-08-31 18:06:49 +0000 +0000'>August 31, 2025</span>&nbsp;·&nbsp;Serhat Giydiren</footer>
  <a class="entry-link" aria-label="post link to AI Safety Diary: August 31, 2025" href="https://serhatgiydiren.com/posts/ai-safety-diary-august-31-2025/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">AI Safety Diary: August 30, 2025
    </h2>
  </header>
  <div class="entry-content">
    <p>Today, I explored two videos from the Anthropic YouTube channel as part of my AI safety studies. Below are the resources I reviewed.
Resource: Tracing the Thoughts of a Large Language Model Source: Tracing the Thoughts of a Large Language Model, Anthropic YouTube channel. Summary: This video discusses Anthropic’s efforts to trace the reasoning processes of large language models (LLMs) like Claude. It explores interpretability techniques to understand how models process inputs and generate outputs, focusing on identifying key computational pathways. The talk emphasizes the importance of transparency in model behavior for ensuring safety and mitigating risks of unintended or harmful outputs. Resource: How Difficult is AI Alignment? | Anthropic Research Salon Source: How Difficult is AI Alignment? | Anthropic Research Salon, Anthropic YouTube channel. Summary: This Research Salon video features Anthropic researchers discussing the challenges of aligning AI systems with human values. It covers technical hurdles, such as ensuring models prioritize safety and ethical behavior, and the complexity of defining robust alignment goals. The discussion highlights ongoing research to address alignment difficulties and reduce risks from advanced AI systems. </p>
  </div>
  <footer class="entry-footer"><span title='2025-08-30 18:06:35 +0000 +0000'>August 30, 2025</span>&nbsp;·&nbsp;Serhat Giydiren</footer>
  <a class="entry-link" aria-label="post link to AI Safety Diary: August 30, 2025" href="https://serhatgiydiren.com/posts/ai-safety-diary-august-30-2025/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">AI Safety Diary: August 29, 2025
    </h2>
  </header>
  <div class="entry-content">
    <p>Today, I explored a research paper as part of my AI safety studies. Below is the resource I reviewed.
Resource: AI Governance to Avoid Extinction: The Strategic Landscape and Actionable Research Questions Source: AI Governance to Avoid Extinction: The Strategic Landscape and Actionable Research Questions by Peter Barnett and Aaron Scher, arXiv:2505.04592, May 2025. Summary: This paper outlines the catastrophic risks of advanced AI, including human extinction from misalignment, misuse, or geopolitical conflict. It proposes four scenarios for AI development, favoring an “Off Switch” and international halt on dangerous AI activities. The authors highlight the need for urgent research into governance mechanisms, such as technical infrastructure for restricting AI development and international agreements to mitigate risks. </p>
  </div>
  <footer class="entry-footer"><span title='2025-08-29 18:00:08 +0000 +0000'>August 29, 2025</span>&nbsp;·&nbsp;Serhat Giydiren</footer>
  <a class="entry-link" aria-label="post link to AI Safety Diary: August 29, 2025" href="https://serhatgiydiren.com/posts/ai-safety-diary-august-29-2025/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">AI Safety Diary: August 28, 2025
    </h2>
  </header>
  <div class="entry-content">
    <p>Today, I explored a research paper as part of my AI safety studies. Below is the resource I reviewed.
Resource: Thought Anchors: Which LLM Reasoning Steps Matter? Source: Thought Anchors: Which LLM Reasoning Steps Matter? by Paul C. Bogdan et al., arXiv:2506.19143, June 2025. Summary: This paper introduces “thought anchors,” key reasoning steps in chain-of-thought (CoT) processes that significantly influence subsequent reasoning. Using three attribution methods (counterfactual importance, attention pattern aggregation, and causal attribution), the study identifies planning or backtracking sentences as critical. These findings enhance interpretability for safety research by pinpointing which CoT steps matter most, with tools provided at thought-anchors.com for visualization. </p>
  </div>
  <footer class="entry-footer"><span title='2025-08-28 17:59:46 +0000 +0000'>August 28, 2025</span>&nbsp;·&nbsp;Serhat Giydiren</footer>
  <a class="entry-link" aria-label="post link to AI Safety Diary: August 28, 2025" href="https://serhatgiydiren.com/posts/ai-safety-diary-august-28-2025/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">AI Safety Diary: August 27, 2025
    </h2>
  </header>
  <div class="entry-content">
    <p>Today, I explored a research paper as part of my AI safety studies. Below is the resource I reviewed.
Resource: Chain-of-Thought Reasoning In The Wild Is Not Always Faithful Source: Chain-of-Thought Reasoning In The Wild Is Not Always Faithful by Iván Arcuschin et al., arXiv:2503.08679, June 2025. Summary: This paper investigates the faithfulness of chain-of-thought (CoT) reasoning in LLMs, finding that models can produce logically contradictory CoT outputs due to implicit biases, termed “Implicit Post-Hoc Rationalization.” For example, models may justify answering “Yes” to both “Is X bigger than Y?” and “Is Y bigger than X?” The study shows high rates of unfaithful reasoning in models like GPT-4o-mini (13%) and Haiku 3.5 (7%), raising challenges for detecting undesired behavior via CoT monitoring. </p>
  </div>
  <footer class="entry-footer"><span title='2025-08-27 17:59:25 +0000 +0000'>August 27, 2025</span>&nbsp;·&nbsp;Serhat Giydiren</footer>
  <a class="entry-link" aria-label="post link to AI Safety Diary: August 27, 2025" href="https://serhatgiydiren.com/posts/ai-safety-diary-august-27-2025/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">AI Safety Diary: August 26, 2025
    </h2>
  </header>
  <div class="entry-content">
    <p>Today, I explored a research paper as part of my AI safety studies. Below is the resource I reviewed.
Resource: Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety Source: Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety by Tomek Korbak et al., arXiv:2507.11473, July 2025. Summary: This paper highlights the potential of monitoring chain-of-thought (CoT) reasoning in large language models (LLMs) to detect misbehavior, such as intent to hack or manipulate. CoT monitoring offers a unique safety opportunity by providing insight into models’ reasoning processes, but it is fragile due to potential optimization pressures that may reduce transparency. The authors recommend further research into CoT monitorability, evaluating its faithfulness, and preserving it through careful model design, as it could complement existing safety measures despite its limitations. </p>
  </div>
  <footer class="entry-footer"><span title='2025-08-26 17:58:57 +0000 +0000'>August 26, 2025</span>&nbsp;·&nbsp;Serhat Giydiren</footer>
  <a class="entry-link" aria-label="post link to AI Safety Diary: August 26, 2025" href="https://serhatgiydiren.com/posts/ai-safety-diary-august-26-2025/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">AI Safety Diary: August 25, 2025
    </h2>
  </header>
  <div class="entry-content">
    <p>Today, I explored a chapter from the Introduction to Effective Altruism Handbook as part of my AI safety and governance studies. Below is the resource I reviewed.
Resource: Our Final Century? Source: Our Final Century?, Effective Altruism Forum, Chapter 4 of the Introduction to Effective Altruism Handbook. Summary: This chapter examines existential risks that could destroy humanity’s long-term potential, emphasizing their moral priority and societal neglect. It focuses on risks like human-made pandemics worse than COVID-19 and discusses strategies for improving biosecurity to prevent catastrophic outcomes. The chapter introduces the concept of “expected value” to evaluate the impact of interventions and explores “hits-based giving,” where high-risk, high-reward approaches are prioritized. It also highlights the importance of identifying crucial considerations to avoid missing key factors that could undermine impact. </p>
  </div>
  <footer class="entry-footer"><span title='2025-08-25 17:55:33 +0000 +0000'>August 25, 2025</span>&nbsp;·&nbsp;Serhat Giydiren</footer>
  <a class="entry-link" aria-label="post link to AI Safety Diary: August 25, 2025" href="https://serhatgiydiren.com/posts/ai-safety-diary-august-25-2025/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">AI Safety Diary: August 24, 2025
    </h2>
  </header>
  <div class="entry-content">
    <p>Today, I explored the audio version of a chapter from the AI Safety Atlas as part of my AI safety studies. Below is the resource I reviewed.
Resource: AI Safety Atlas (Chapter 4: Governance Audio) Source: Chapter 4: Governance, AI Safety Atlas by Markov Grey and Charbel-Raphaël Segerie et al., French Center for AI Safety (CeSIA), 2025. Summary: The audio version of this chapter focuses on governance strategies for ensuring the safe development and deployment of advanced AI systems. It explores frameworks such as safety standards, international treaties, and regulatory policies to manage AI risks. The chapter discusses the trade-offs between centralized and decentralized approaches to AI access, the role of stakeholder collaboration, and the importance of establishing robust oversight mechanisms to align AI systems with societal values and prevent misuse or unintended consequences. </p>
  </div>
  <footer class="entry-footer"><span title='2025-08-24 17:51:12 +0000 +0000'>August 24, 2025</span>&nbsp;·&nbsp;Serhat Giydiren</footer>
  <a class="entry-link" aria-label="post link to AI Safety Diary: August 24, 2025" href="https://serhatgiydiren.com/posts/ai-safety-diary-august-24-2025/"></a>
</article>
<footer class="page-footer">
  <nav class="pagination">
    <a class="next" href="https://serhatgiydiren.com/categories/ai-safety/page/2/">Next&nbsp;&nbsp;»
    </a>
  </nav>
</footer>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://serhatgiydiren.com/">Serhat Giydiren</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
