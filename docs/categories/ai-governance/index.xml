<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI Governance on Serhat Giydiren Blog</title>
    <link>https://serhatgiydiren.github.io/categories/ai-governance/</link>
    <description>Recent content in AI Governance on Serhat Giydiren Blog</description>
    <generator>Hugo</generator>
    <language>tr-tr</language>
    <lastBuildDate>Fri, 15 Aug 2025 12:40:49 +0000</lastBuildDate>
    <atom:link href="https://serhatgiydiren.github.io/categories/ai-governance/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>AI Safety Diary: August 15, 2025</title>
      <link>https://serhatgiydiren.github.io/posts/ai-safety-diary-august-15-2025/</link>
      <pubDate>Fri, 15 Aug 2025 12:40:49 +0000</pubDate>
      <guid>https://serhatgiydiren.github.io/posts/ai-safety-diary-august-15-2025/</guid>
      <description>&lt;p&gt;Today, I continued exploring the &lt;a href=&#34;https://forum.effectivealtruism.org/handbook&#34;&gt;Effective Altruism Handbook&lt;/a&gt; and completed its second chapter as part of my studies related to AI safety and governance. Below is the resource I reviewed.&lt;/p&gt;&#xA;&lt;h2 id=&#34;resource-differences-in-impact&#34;&gt;Resource: Differences in Impact&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Source&lt;/strong&gt;: &lt;a href=&#34;https://forum.effectivealtruism.org/s/x3KXkiAQ6NH8WLbkW&#34;&gt;Differences in Impact&lt;/a&gt;, Effective Altruism Forum, Chapter 2 of the Effective Altruism Handbook.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Summary&lt;/strong&gt;: This chapter focuses on the significant disparities in the effectiveness of interventions aimed at helping the approximately 700 million people living in poverty, primarily in low-income countries. It discusses strategies such as policy reform, cash transfers, and health service provision, emphasizing that some interventions are far more effective than others. The chapter introduces a simple tool for estimating key figures to evaluate impact and includes recommended readings, such as GiveWell&amp;rsquo;s &amp;ldquo;Giving 101&amp;rdquo; guide and sections on global health outcomes, to illustrate effective altruism approaches to addressing global poverty.&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>AI Safety Diary: August 12, 2025</title>
      <link>https://serhatgiydiren.github.io/posts/ai-safety-diary-august-12-2025/</link>
      <pubDate>Tue, 12 Aug 2025 12:24:42 +0000</pubDate>
      <guid>https://serhatgiydiren.github.io/posts/ai-safety-diary-august-12-2025/</guid>
      <description>&lt;p&gt;Today, I completed Unit 1: How AI Systems Work of the &lt;a href=&#34;https://bluedot.org/courses/governance/1&#34;&gt;BlueDot AI Governance course&lt;/a&gt;. Below is a summary of each resource I explored.&lt;/p&gt;&#xA;&lt;h2 id=&#34;resource-how-does-ai-learn-a-beginners-guide-with-examples&#34;&gt;Resource: How Does AI Learn? A Beginner’s Guide with Examples&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Source&lt;/strong&gt;: &lt;a href=&#34;https://aisafetyfundamentals.com/blog/how-does-ai-learn&#34;&gt;How Does AI Learn? A Beginner’s Guide with Examples&lt;/a&gt;, AI Safety Fundamentals.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Summary&lt;/strong&gt;: This guide provides an accessible introduction to how AI systems learn, focusing on machine learning. It explains key concepts like supervised learning (where models learn from labeled data, e.g., identifying spam emails), unsupervised learning (finding patterns in unlabeled data, e.g., clustering customer preferences), and reinforcement learning (learning through rewards, e.g., training a game-playing AI). The article uses simple examples to illustrate how models are trained and highlights challenges like overfitting and the need for diverse datasets to avoid bias.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;resource-large-language-models-explained-briefly&#34;&gt;Resource: Large Language Models Explained Briefly&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Source&lt;/strong&gt;: &lt;a href=&#34;https://youtu.be/LPZh9BOjkQs&#34;&gt;Large Language Models Explained Briefly&lt;/a&gt;, YouTube video.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Summary&lt;/strong&gt;: This short video offers a concise overview of large language models (LLMs). It explains that LLMs, like GPT-3, are trained on vast text datasets to predict the next word in a sequence, enabling them to generate coherent text. The video covers their architecture, primarily transformers, and their applications, such as chatbots and text generation. It also touches on limitations, including high computational costs and potential biases in training data.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;resource-intro-to-large-language-models&#34;&gt;Resource: Intro to Large Language Models&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Source&lt;/strong&gt;: &lt;a href=&#34;https://youtu.be/zjkBMFhNj_g&#34;&gt;Intro to Large Language Models&lt;/a&gt;, YouTube video by Andrej Karpathy.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Summary&lt;/strong&gt;: This one-hour talk provides a detailed introduction to large language models (LLMs). It explains how LLMs are built on transformer architectures and trained on massive text corpora to perform tasks like text generation, translation, and question-answering. The video discusses the importance of scaling compute and data for performance improvements, the emergence of capabilities like reasoning, and challenges such as alignment with human values and mitigating harmful outputs.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;resource-visualizing-the-deep-learning-revolution&#34;&gt;Resource: Visualizing the Deep Learning Revolution&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Source&lt;/strong&gt;: &lt;a href=&#34;https://medium.com/@richardcngo/visualizing-the-deep-learning-revolution-722098eb9c5&#34;&gt;Visualizing the Deep Learning Revolution&lt;/a&gt; by Richard Ngo, Medium.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Summary&lt;/strong&gt;: This article illustrates the rapid progress in AI over the past decade, driven by deep learning. It covers advancements in four domains: vision (e.g., image and video generation with GANs, transformers, and diffusion models), games (e.g., AlphaGo, AlphaStar, and AI in open-ended environments like Minecraft), language-based tasks (e.g., GPT-2, GPT-3, and ChatGPT’s text generation and reasoning capabilities), and science (e.g., AlphaFold 2 solving protein folding and AI generating chemical compounds). The article emphasizes that scaling compute and data, rather than new algorithms, has been the primary driver of progress. It also notes that AI’s rapid advancement has surprised experts and raises concerns about existential risks from unaligned AGI.&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>AI Safety Diary: August 11, 2025</title>
      <link>https://serhatgiydiren.github.io/posts/ai-safety-diary-august-11-2025/</link>
      <pubDate>Mon, 11 Aug 2025 20:36:43 +0000</pubDate>
      <guid>https://serhatgiydiren.github.io/posts/ai-safety-diary-august-11-2025/</guid>
      <description>&lt;p&gt;Today, I began Unit 1: How AI Systems Work of the &lt;a href=&#34;https://bluedot.org/courses/governance/1&#34;&gt;BlueDot AI Governance course&lt;/a&gt;. Below is the resource I explored.&lt;/p&gt;&#xA;&lt;h2 id=&#34;resource-the-ai-triad-and-what-it-means-for-national-security-strategy&#34;&gt;Resource: The AI Triad and What It Means for National Security Strategy&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Source&lt;/strong&gt;: &lt;a href=&#34;https://cset.georgetown.edu/wp-content/uploads/CSET-AI-Triad-Report.pdf&#34;&gt;The AI Triad and What It Means for National Security Strategy&lt;/a&gt; by Ben Buchanan, Center for Security and Emerging Technology (CSET), August 2020.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Summary&lt;/strong&gt;: This paper introduces the &amp;ldquo;AI Triad&amp;rdquo; framework—algorithms, data, and computing power—to explain modern machine learning and its implications for national security. It describes algorithms as instructions for processing information, covering supervised learning (predicting outcomes from labeled data), unsupervised learning (finding patterns in unorganized data), and reinforcement learning (learning through trial and error). Data is critical for training AI systems, particularly for supervised learning, but requires careful management to avoid bias and address privacy concerns. Computing power is highlighted as a key driver of AI progress, with a 300,000-fold increase in compute used for top AI projects from 2012 to 2018. The paper connects these components to national security applications, such as analyzing drone footage, targeting propaganda, and powering autonomous military vehicles. It also discusses policy levers like talent recruitment for algorithms, privacy regulations for data, and export controls for compute.&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>AI Safety Diary: August 10, 2025</title>
      <link>https://serhatgiydiren.github.io/posts/ai-safety-diary-august-10-2025/</link>
      <pubDate>Sun, 10 Aug 2025 12:47:45 +0000</pubDate>
      <guid>https://serhatgiydiren.github.io/posts/ai-safety-diary-august-10-2025/</guid>
      <description>&lt;p&gt;Today, I continued exploring the &lt;a href=&#34;https://aisafetybook.com&#34;&gt;Introduction to AI Safety, Ethics, and Society&lt;/a&gt; textbook as part of my AI safety studies. Below is the resource I reviewed.&lt;/p&gt;&#xA;&lt;h2 id=&#34;resource-introduction-to-ai-safety-ethics-and-society-chapters-610-slides&#34;&gt;Resource: Introduction to AI Safety, Ethics, and Society (Chapters 6–10 Slides)&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Source&lt;/strong&gt;: &lt;a href=&#34;https://aisafetybook.com&#34;&gt;Introduction to AI Safety, Ethics, and Society&lt;/a&gt; by Dan Hendrycks, Taylor &amp;amp; Francis, 2024.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Summary&lt;/strong&gt;: The slides for chapters 6–10 of this textbook, developed by Dan Hendrycks, director of the Center for AI Safety, conclude the introduction to AI safety, ethics, and societal impacts. The chapters covered are:&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Chapter 6: Beneficial AI and Machine Ethics&lt;/strong&gt; - Explores the design of AI systems that align with human values and ethical principles, discussing frameworks for ensuring AI contributes positively to society.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Chapter 7: Collective Action Problems&lt;/strong&gt; - Examines challenges in coordinating AI development across stakeholders, addressing issues like competition and cooperation that impact safe AI deployment.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Chapter 8: Governance&lt;/strong&gt; - Covers approaches to AI governance, including safety standards, international treaties, and trade-offs between centralized and decentralized access to advanced AI systems.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Chapter 9: Appendix: Ethics&lt;/strong&gt; - Provides additional insights into ethical considerations for AI, focusing on moral frameworks and their application to AI decision-making.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Chapter 10: Appendix: Utility Functions&lt;/strong&gt; - Discusses the role of utility functions in AI systems, exploring how they shape AI behavior and the challenges of defining safe and effective objectives.&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>AI Safety Diary: August 9, 2025</title>
      <link>https://serhatgiydiren.github.io/posts/ai-safety-diary-august-9-2025/</link>
      <pubDate>Sat, 09 Aug 2025 12:45:43 +0000</pubDate>
      <guid>https://serhatgiydiren.github.io/posts/ai-safety-diary-august-9-2025/</guid>
      <description>&lt;p&gt;Today, I explored the &lt;a href=&#34;https://aisafetybook.com&#34;&gt;Introduction to AI Safety, Ethics, and Society&lt;/a&gt; textbook as part of my AI safety studies. Below is the resource I reviewed.&lt;/p&gt;&#xA;&lt;h2 id=&#34;resource-introduction-to-ai-safety-ethics-and-society-chapters-15-slides&#34;&gt;Resource: Introduction to AI Safety, Ethics, and Society (Chapters 1–5 Slides)&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Source&lt;/strong&gt;: &lt;a href=&#34;https://aisafetybook.com&#34;&gt;Introduction to AI Safety, Ethics, and Society&lt;/a&gt; by Dan Hendrycks, Taylor &amp;amp; Francis, 2024.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Summary&lt;/strong&gt;: The slides for the first five chapters of this textbook, developed by Dan Hendrycks, director of the Center for AI Safety, provide an introduction to AI safety, ethics, and societal impacts. The chapters covered are:&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Chapter 1: Overview of Catastrophic AI Risks&lt;/strong&gt; - Introduces potential catastrophic risks from advanced AI, such as malicious use, accidents, and rogue AI systems.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Chapter 2: AI Fundamentals&lt;/strong&gt; - Covers the basics of modern AI systems, focusing on deep learning, transformer architectures, and scaling laws that drive AI performance.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Chapter 3: Single-Agent Safety&lt;/strong&gt; - Discusses technical challenges in ensuring the safety of individual AI systems, including issues like opaqueness, proxy gaming, and adversarial attacks.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Chapter 4: Safety Engineering&lt;/strong&gt; - Explores principles of safety engineering applied to AI, emphasizing methods to design robust and reliable AI systems.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Chapter 5: Complex Systems&lt;/strong&gt; - Examines AI within the context of complex sociotechnical systems, highlighting the role of systems theory in managing risks from AI deployment.&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>AI Safety Diary: August 8, 2025</title>
      <link>https://serhatgiydiren.github.io/posts/ai-safety-diary-august-8-2025/</link>
      <pubDate>Fri, 08 Aug 2025 12:38:05 +0000</pubDate>
      <guid>https://serhatgiydiren.github.io/posts/ai-safety-diary-august-8-2025/</guid>
      <description>&lt;p&gt;Today, I explored the &lt;a href=&#34;https://forum.effectivealtruism.org/handbook&#34;&gt;Effective Altruism Handbook&lt;/a&gt; and completed its first chapter as part of my studies related to AI safety and governance. Below is the resource I reviewed.&lt;/p&gt;&#xA;&lt;h2 id=&#34;resource-the-effectiveness-mindset&#34;&gt;Resource: The Effectiveness Mindset&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Source&lt;/strong&gt;: &lt;a href=&#34;https://forum.effectivealtruism.org/s/B79ro5zkhndbBKRRX&#34;&gt;The Effectiveness Mindset&lt;/a&gt;, Effective Altruism Forum, Chapter 1 of the Effective Altruism Handbook.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Summary&lt;/strong&gt;: This chapter introduces the core idea of effective altruism: maximizing the impact of one&amp;rsquo;s time and resources to help others. It emphasizes the importance of focusing on interventions that benefit the most people, rather than those with lesser impact. The chapter highlights the challenge of identifying effective interventions, which requires a &amp;ldquo;scout mindset&amp;rdquo;—an approach focused on seeking truth and questioning existing ideas rather than defending preconceived notions.&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>
