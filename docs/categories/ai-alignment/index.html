<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>AI Alignment | Serhat Giydiren</title>
<meta name="keywords" content="">
<meta name="description" content="">
<meta name="author" content="">
<link rel="canonical" href="https://serhatgiydiren.com/categories/ai-alignment/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.2211ca3164be7830024f6aad2b3a2e520843a64f8f048445c3401c1249aa051d.css" integrity="sha256-IhHKMWS&#43;eDACT2qtKzouUghDpk&#43;PBIRFw0AcEkmqBR0=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://serhatgiydiren.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://serhatgiydiren.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://serhatgiydiren.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://serhatgiydiren.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://serhatgiydiren.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="https://serhatgiydiren.com/categories/ai-alignment/index.xml">
<link rel="alternate" hreflang="en" href="https://serhatgiydiren.com/categories/ai-alignment/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="https://serhatgiydiren.com/categories/ai-alignment/">
  <meta property="og:site_name" content="Serhat Giydiren">
  <meta property="og:title" content="AI Alignment">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="website">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="AI Alignment">
<meta name="twitter:description" content="">

</head>

<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://serhatgiydiren.com/" accesskey="h" title="Serhat Giydiren (Alt + H)">Serhat Giydiren</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://serhatgiydiren.com/archives" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://serhatgiydiren.com/search" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main"> 
<header class="page-header">
  <h1>
    AI Alignment
  </h1>
</header>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">AI Safety Diary: August 15, 2025
    </h2>
  </header>
  <div class="entry-content">
    <p>Today, I continued exploring the Effective Altruism Handbook and completed its second chapter as part of my studies related to AI safety and governance. Below is the resource I reviewed.
Resource: Differences in Impact Source: Differences in Impact , Effective Altruism Forum, Chapter 2 of the Effective Altruism Handbook. Summary: This chapter focuses on the significant disparities in the effectiveness of interventions aimed at helping the approximately 700 million people living in poverty, primarily in low-income countries. It discusses strategies such as policy reform, cash transfers, and health service provision, emphasizing that some interventions are far more effective than others. The chapter introduces a simple tool for estimating key figures to evaluate impact and includes recommended readings, such as GiveWell’s “Giving 101” guide and sections on global health outcomes, to illustrate effective altruism approaches to addressing global poverty. </p>
  </div>
  <footer class="entry-footer"><span title='2025-08-15 12:40:49 +0000 +0000'>August 15, 2025</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Serhat Giydiren</footer>
  <a class="entry-link" aria-label="post link to AI Safety Diary: August 15, 2025" href="https://serhatgiydiren.com/ai-safety-diary-august-15-2025/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">AI Safety Diary: August 14, 2025
    </h2>
  </header>
  <div class="entry-content">
    <p>Today, I explored the AI Safety Atlas as part of my AI safety studies. Below is the resource I reviewed.
Resource: AI Safety Atlas (Chapter 1: Capabilities) Source: Chapter 1: Capabilities - Video Lecture (AI is Advancing Faster Than You Think! (AI Safety symposium 2/5)) , AI Safety Atlas by Markov Grey and Charbel-Raphaël Segerie et al., French Center for AI Safety (CeSIA), 2025. Summary: This chapter provides an overview of AI capabilities, focusing on the progression of modern AI systems toward artificial general intelligence (AGI). It discusses the increasing power of foundation models, such as large language models, and the importance of defining and measuring intelligence for safety purposes. The chapter explores challenges in defining intelligence, comparing approaches like the Turing Test, consciousness-based definitions, process-based adaptability, and a capabilities-focused framework. It emphasizes the latter, which assesses what AI systems can do, their performance levels, and the range of tasks they can handle, as the most practical for safety evaluations. The chapter also introduces frameworks for measuring AI progress on a continuous spectrum, moving beyond binary distinctions like narrow versus general AI, to better understand capabilities and associated risks. </p>
  </div>
  <footer class="entry-footer"><span title='2025-08-14 12:55:39 +0000 +0000'>August 14, 2025</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Serhat Giydiren</footer>
  <a class="entry-link" aria-label="post link to AI Safety Diary: August 14, 2025" href="https://serhatgiydiren.com/ai-safety-diary-august-14-2025/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">AI Safety Diary: August 13, 2025
    </h2>
  </header>
  <div class="entry-content">
    <p>Today, I began the BlueDot AI Alignment course and completed its first unit as part of my AI safety studies. Below is the resource I reviewed.
Resource: AI and the Years Ahead Source: Unit 1: AI and the Years Ahead , BlueDot Impact AI Alignment Course. Summary: This unit introduces the foundational concepts of AI and its potential future impacts. It describes AI as a collection of approaches, focusing on key techniques like neural networks, gradient descent, and transformers used to train large language models (LLMs) such as ChatGPT. The unit explains how hardware advancements have driven AI progress and covers essential machine learning terms like weights, biases, parameters, neurons, and activations. It also explores the economic and non-economic incentives behind developing transformative AI systems and highlights recent advances in AI capabilities, providing a framework for understanding AI’s societal and economic implications. </p>
  </div>
  <footer class="entry-footer"><span title='2025-08-13 12:53:05 +0000 +0000'>August 13, 2025</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Serhat Giydiren</footer>
  <a class="entry-link" aria-label="post link to AI Safety Diary: August 13, 2025" href="https://serhatgiydiren.com/ai-safety-diary-august-13-2025/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">AI Safety Diary: August 10, 2025
    </h2>
  </header>
  <div class="entry-content">
    <p>Today, I continued exploring the Introduction to AI Safety, Ethics, and Society textbook as part of my AI safety studies. Below is the resource I reviewed.
Resource: Introduction to AI Safety, Ethics, and Society (Chapters 6–10 Slides) Source: Introduction to AI Safety, Ethics, and Society by Dan Hendrycks, Taylor &amp; Francis, 2024. Summary: The slides for chapters 6–10 of this textbook, developed by Dan Hendrycks, director of the Center for AI Safety, conclude the introduction to AI safety, ethics, and societal impacts. The chapters covered are: Chapter 6: Beneficial AI and Machine Ethics - Explores the design of AI systems that align with human values and ethical principles, discussing frameworks for ensuring AI contributes positively to society. Chapter 7: Collective Action Problems - Examines challenges in coordinating AI development across stakeholders, addressing issues like competition and cooperation that impact safe AI deployment. Chapter 8: Governance - Covers approaches to AI governance, including safety standards, international treaties, and trade-offs between centralized and decentralized access to advanced AI systems. Chapter 9: Appendix: Ethics - Provides additional insights into ethical considerations for AI, focusing on moral frameworks and their application to AI decision-making. Chapter 10: Appendix: Utility Functions - Discusses the role of utility functions in AI systems, exploring how they shape AI behavior and the challenges of defining safe and effective objectives. </p>
  </div>
  <footer class="entry-footer"><span title='2025-08-10 12:47:45 +0000 +0000'>August 10, 2025</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;Serhat Giydiren</footer>
  <a class="entry-link" aria-label="post link to AI Safety Diary: August 10, 2025" href="https://serhatgiydiren.com/ai-safety-diary-august-10-2025/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">AI Safety Diary: August 9, 2025
    </h2>
  </header>
  <div class="entry-content">
    <p>Today, I explored the Introduction to AI Safety, Ethics, and Society textbook as part of my AI safety studies. Below is the resource I reviewed.
Resource: Introduction to AI Safety, Ethics, and Society (Chapters 1–5 Slides) Source: Introduction to AI Safety, Ethics, and Society by Dan Hendrycks, Taylor &amp; Francis, 2024. Summary: The slides for the first five chapters of this textbook, developed by Dan Hendrycks, director of the Center for AI Safety, provide an introduction to AI safety, ethics, and societal impacts. The chapters covered are: Chapter 1: Overview of Catastrophic AI Risks - Introduces potential catastrophic risks from advanced AI, such as malicious use, accidents, and rogue AI systems. Chapter 2: AI Fundamentals - Covers the basics of modern AI systems, focusing on deep learning, transformer architectures, and scaling laws that drive AI performance. Chapter 3: Single-Agent Safety - Discusses technical challenges in ensuring the safety of individual AI systems, including issues like opaqueness, proxy gaming, and adversarial attacks. Chapter 4: Safety Engineering - Explores principles of safety engineering applied to AI, emphasizing methods to design robust and reliable AI systems. Chapter 5: Complex Systems - Examines AI within the context of complex sociotechnical systems, highlighting the role of systems theory in managing risks from AI deployment. </p>
  </div>
  <footer class="entry-footer"><span title='2025-08-09 12:45:43 +0000 +0000'>August 9, 2025</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Serhat Giydiren</footer>
  <a class="entry-link" aria-label="post link to AI Safety Diary: August 9, 2025" href="https://serhatgiydiren.com/ai-safety-diary-august-9-2025/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">AI Safety Diary: August 8, 2025
    </h2>
  </header>
  <div class="entry-content">
    <p>Today, I explored the Effective Altruism Handbook and completed its first chapter as part of my studies related to AI safety and governance. Below is the resource I reviewed.
Resource: The Effectiveness Mindset Source: The Effectiveness Mindset , Effective Altruism Forum, Chapter 1 of the Effective Altruism Handbook. Summary: This chapter introduces the core idea of effective altruism: maximizing the impact of one’s time and resources to help others. It emphasizes the importance of focusing on interventions that benefit the most people, rather than those with lesser impact. The chapter highlights the challenge of identifying effective interventions, which requires a “scout mindset”—an approach focused on seeking truth and questioning existing ideas rather than defending preconceived notions. </p>
  </div>
  <footer class="entry-footer"><span title='2025-08-08 12:38:05 +0000 +0000'>August 8, 2025</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Serhat Giydiren</footer>
  <a class="entry-link" aria-label="post link to AI Safety Diary: August 8, 2025" href="https://serhatgiydiren.com/ai-safety-diary-august-8-2025/"></a>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://serhatgiydiren.com/">Serhat Giydiren</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
